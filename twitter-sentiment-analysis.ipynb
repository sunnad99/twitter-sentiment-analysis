{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n\n# Text preprocessing\nfrom nltk.tokenize import TweetTokenizer, RegexpTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer\nimport re\n\n# Building a model\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.optimizers import Adam\nimport matplotlib.pyplot as plt\n\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-25T17:22:14.170906Z","iopub.execute_input":"2021-09-25T17:22:14.171312Z","iopub.status.idle":"2021-09-25T17:22:16.919357Z","shell.execute_reply.started":"2021-09-25T17:22:14.171265Z","shell.execute_reply":"2021-09-25T17:22:16.918304Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Step 1: Load all the data from the dataset csv file (1.6 millions tweets)        \ndataset_filepath = \"../input/sentiment140/training.1600000.processed.noemoticon.csv\"\nDATASET_ENCODING = \"ISO-8859-1\"\nDATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n\n\ntwitter_df = pd.read_csv(dataset_filepath, encoding = DATASET_ENCODING, names = DATASET_COLUMNS, header = None)\nlabels = twitter_df.iloc[:, 0]\nsentences = twitter_df.iloc[:, -1]\ntwitter_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T17:22:16.922033Z","iopub.execute_input":"2021-09-25T17:22:16.922865Z","iopub.status.idle":"2021-09-25T17:22:21.177593Z","shell.execute_reply.started":"2021-09-25T17:22:16.922813Z","shell.execute_reply":"2021-09-25T17:22:21.176420Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Step 2: Preprocessing of sentences to handle\n# 1. Removing URLs\n# 2. Removing Emails\n# 3. Remove new lines characters\n# 4. Remove distracting single quotes\n# 5. Remove capitalization of words\n# 6. Remove all twitter handles\n# 7. Removal of stop words\n# 8. Remove punctuation\n\n\ntweet_tokenizer = TweetTokenizer(strip_handles=True)\npunct_tokenizer = RegexpTokenizer(r'\\w+')\ndetokenizer = TreebankWordDetokenizer()\nstop_words = set(stopwords.words('english'))\n\ndef depure_data(data): \n    \n    global tweet_tokenizer, stop_words, punct_tokenizer, detokenizer\n    \n    #Removing URLs with a regular expression\n    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    data = url_pattern.sub(r'', data)\n\n    # Remove Emails\n    data = re.sub('\\S*@\\S*\\s?', '', data)\n\n    # Remove new line characters\n    data = re.sub('\\s+', ' ', data)\n\n    # Remove distracting single quotes\n    data = re.sub(\"\\'\", \"\", data)\n    \n    # Remove capitalization of words\n    data = data.lower()\n    \n    # Removal of twitter handles\n    word_list =  tweet_tokenizer.tokenize(data)\n    \n    # Removal of stop words\n    #filtered_stop_words = [word for word in word_list if word not in stop_words]\n    data = detokenizer.detokenize(word_list)\n    \n    # Remove punctuation\n    filtered_punct_words = punct_tokenizer.tokenize(data)\n    data = detokenizer.detokenize(filtered_punct_words)\n    \n    \n    return data\n","metadata":{"execution":{"iopub.status.busy":"2021-09-25T17:22:21.179485Z","iopub.execute_input":"2021-09-25T17:22:21.180014Z","iopub.status.idle":"2021-09-25T17:22:21.194528Z","shell.execute_reply.started":"2021-09-25T17:22:21.179968Z","shell.execute_reply":"2021-09-25T17:22:21.193095Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"sentences = sentences.apply(lambda x: depure_data(x))\nsentences.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T17:22:21.196615Z","iopub.execute_input":"2021-09-25T17:22:21.197656Z","iopub.status.idle":"2021-09-25T17:33:52.729520Z","shell.execute_reply.started":"2021-09-25T17:22:21.197609Z","shell.execute_reply":"2021-09-25T17:33:52.728465Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"sentences = list(sentences)\n\nlabels = np.array(list(labels))\n\n# Changing the 4s to 1s to make training easier\nfour_indices = np.where(labels == 4)\nlabels[four_indices[0]] = 1","metadata":{"execution":{"iopub.status.busy":"2021-09-25T17:33:52.731327Z","iopub.execute_input":"2021-09-25T17:33:52.731916Z","iopub.status.idle":"2021-09-25T17:33:53.412476Z","shell.execute_reply.started":"2021-09-25T17:33:52.731868Z","shell.execute_reply":"2021-09-25T17:33:53.411322Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Step 2: We load up the words for tokenization and the vectors for words embeddings\nglove_vector_file_path = \"../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt\"\n\nwords = []\nembeddings_index = {}\n\nwith open(glove_vector_file_path, \"r\") as GVF:\n    for current_row,row in enumerate(GVF):\n        line = row.split()\n        \n        word = line[0]\n        coefs = np.asarray(line[1:], dtype = \"float32\")\n        \n        embeddings_index[word] = coefs\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2021-09-25T17:33:53.418438Z","iopub.execute_input":"2021-09-25T17:33:53.418985Z","iopub.status.idle":"2021-09-25T17:34:12.456766Z","shell.execute_reply.started":"2021-09-25T17:33:53.418936Z","shell.execute_reply":"2021-09-25T17:34:12.455833Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Intermediate Step 1: Defining all the hyper parameters\ntrunc_type = \"post\"\npad_type = \"post\"\noov_tok = \"<OOV>\"\n\nembedding_dim = 100\nmax_len = 50\n\ntrain_batch_size =  2**10 # 1024 samples per batch (1,280,000 total train samples)\nval_batch_size = 2**7 # 128 samples per batch (160,000 total validation samples)\nno_of_epochs = 50","metadata":{"execution":{"iopub.status.busy":"2021-09-25T17:34:12.458402Z","iopub.execute_input":"2021-09-25T17:34:12.458836Z","iopub.status.idle":"2021-09-25T17:34:12.469006Z","shell.execute_reply.started":"2021-09-25T17:34:12.458794Z","shell.execute_reply":"2021-09-25T17:34:12.467195Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Step 3: Carry out tokenization of words and convert into integer sequences\nword_tokenizer = Tokenizer(oov_token = oov_tok)\nword_tokenizer.fit_on_texts(sentences)\n\nprint(len(word_tokenizer.word_index))\n\ntotal_sequences = word_tokenizer.texts_to_sequences(sentences)\ndata = pad_sequences(total_sequences, padding = pad_type, truncating = trunc_type, maxlen = max_len)\n\nprint(len(data))\n","metadata":{"execution":{"iopub.status.busy":"2021-09-25T17:37:19.127614Z","iopub.execute_input":"2021-09-25T17:37:19.128038Z","iopub.status.idle":"2021-09-25T17:38:36.595762Z","shell.execute_reply.started":"2021-09-25T17:37:19.128005Z","shell.execute_reply":"2021-09-25T17:38:36.594618Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Step 4: We split the data into a train and test set\ntrain_portion = 0.8\n\nindices = np.arange(len(data))\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = labels[indices]\n\ntrain_size = int(0.8*len(data))\nperformance_checking_size = len(data) - train_size\nval_and_test_size = int(0.5 * performance_checking_size)\n\ntrain_sentences = data[:train_size]\ntrain_labels = labels[:train_size]\n\nvalidation_sentences = data[train_size: train_size + val_and_test_size]\nvalidation_labels = labels[train_size: train_size + val_and_test_size]\n\ntest_sentences = data[train_size + val_and_test_size: train_size + val_and_test_size * 2]\ntest_labels = labels[train_size + val_and_test_size: train_size + val_and_test_size * 2]\n\nprint(len(train_sentences))\nprint(len(validation_sentences))\nprint(len(test_sentences))","metadata":{"execution":{"iopub.status.busy":"2021-09-25T17:38:36.598654Z","iopub.execute_input":"2021-09-25T17:38:36.599346Z","iopub.status.idle":"2021-09-25T17:38:36.881312Z","shell.execute_reply.started":"2021-09-25T17:38:36.599298Z","shell.execute_reply":"2021-09-25T17:38:36.879911Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Step 5: We create the weights matrix for the embedding layer\nembedding_vocab_size = len(word_tokenizer.word_index) + 1\n\nembedding_matrix = np.zeros((embedding_vocab_size, embedding_dim))\nfor word, i in word_tokenizer.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2021-09-25T17:38:36.882995Z","iopub.execute_input":"2021-09-25T17:38:36.883656Z","iopub.status.idle":"2021-09-25T17:38:37.279855Z","shell.execute_reply.started":"2021-09-25T17:38:36.883610Z","shell.execute_reply":"2021-09-25T17:38:37.278799Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Step 6: Create a callback function to early stop the training when a desired accuracy is reached\nclass myCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs = {}):\n        if logs[\"val_accuracy\"] > 0.82:\n            model.stop_training = True\n            print(f\"\\n Stopping training...desired validation accuracy of {logs['val_accuracy'] * 100}% reached!\")\n            \ncallback = myCallback()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T18:11:22.349008Z","iopub.execute_input":"2021-09-25T18:11:22.349379Z","iopub.status.idle":"2021-09-25T18:11:22.358225Z","shell.execute_reply.started":"2021-09-25T18:11:22.349345Z","shell.execute_reply":"2021-09-25T18:11:22.356012Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Step 7: Define the model and compile it\n\nmodel = tf.keras.Sequential([\n                            tf.keras.layers.Embedding(embedding_vocab_size, embedding_dim, weights = [embedding_matrix], trainable = False, input_length = max_len),\n                            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(40)),\n                            \n                            tf.keras.layers.Dense(128, activation = \"relu\"),\n                            tf.keras.layers.Dense(2, activation = \"sigmoid\")\n])\n\nmodel.compile(loss = \"sparse_categorical_crossentropy\", optimizer = Adam(), metrics = [\"accuracy\"])\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T17:41:46.042689Z","iopub.execute_input":"2021-09-25T17:41:46.043105Z","iopub.status.idle":"2021-09-25T17:41:47.068769Z","shell.execute_reply.started":"2021-09-25T17:41:46.043044Z","shell.execute_reply":"2021-09-25T17:41:47.067849Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Step 8: Train the model over a certain number of epochs\nhistory = model.fit(train_sentences, train_labels, batch_size = train_batch_size,\n                    validation_data = (validation_sentences, validation_labels), validation_batch_size = val_batch_size,\n                    epochs = no_of_epochs, callbacks = [callback])\n","metadata":{"execution":{"iopub.status.busy":"2021-09-25T18:11:35.999781Z","iopub.execute_input":"2021-09-25T18:11:36.000136Z","iopub.status.idle":"2021-09-25T18:12:07.515858Z","shell.execute_reply.started":"2021-09-25T18:11:36.000104Z","shell.execute_reply":"2021-09-25T18:12:07.514583Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Step 9: Evaluate the model on a test set\nresults = model.evaluate(test_sentences, test_labels, batch_size = 128)\nprint(f\"Test Loss: {results[0]}, Test Accuracy: {results[1]}\")","metadata":{"execution":{"iopub.status.busy":"2021-09-25T18:12:32.399809Z","iopub.execute_input":"2021-09-25T18:12:32.400179Z","iopub.status.idle":"2021-09-25T18:12:39.215756Z","shell.execute_reply.started":"2021-09-25T18:12:32.400146Z","shell.execute_reply":"2021-09-25T18:12:39.214487Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Bonus step: Testing model on your own data\ntemp_sentence = [\"I hate this world so much that I want to tear it all apart!\", #0\n                 \"What a marvelous exhibit this is!\", #1\n                 \"You broke my trust!\", #0\n                \"you are looking interestingly bad\"] #0\ntemp_sequence = word_tokenizer.texts_to_sequences(temp_sentence)\ntemp_data = pad_sequences(temp_sequence, padding = pad_type, truncating = trunc_type, maxlen = max_len)\n\nmodel.predict(temp_data)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T18:16:03.818596Z","iopub.execute_input":"2021-09-25T18:16:03.819179Z","iopub.status.idle":"2021-09-25T18:16:03.881555Z","shell.execute_reply.started":"2021-09-25T18:16:03.819145Z","shell.execute_reply":"2021-09-25T18:16:03.880515Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# Step 10: Plot the loss and accuracy to visual how well the model performs\nval_loss = history.history[\"val_loss\"]\nloss = history.history[\"loss\"]\n\nx = np.arange(len(val_loss))\nplt.plot(x, loss)\nplt.plot(x, val_loss)\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T18:13:01.037322Z","iopub.execute_input":"2021-09-25T18:13:01.037815Z","iopub.status.idle":"2021-09-25T18:13:01.269765Z","shell.execute_reply.started":"2021-09-25T18:13:01.037773Z","shell.execute_reply":"2021-09-25T18:13:01.268239Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# Final step: Saving the model\nmodel.save(\"./bidirectional_LSTM_layer_model_finalized.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-09-25T18:13:29.551118Z","iopub.execute_input":"2021-09-25T18:13:29.551538Z","iopub.status.idle":"2021-09-25T18:13:29.950686Z","shell.execute_reply.started":"2021-09-25T18:13:29.551505Z","shell.execute_reply":"2021-09-25T18:13:29.949608Z"},"trusted":true},"execution_count":29,"outputs":[]}]}